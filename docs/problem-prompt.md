Background: I have this Chat component. It is meant to interact with a LLM. When it opens, it shows up a greeting. User enter a prompt (we will refer to as userQuery). This goes to the backend. The backend runs a pipeline for prompt optimization. For each step, it should stream back through websocket to the client. Current Behavior: both the userQuery and the bot reply gets updated with streamed data. Gibberish data is also being streamed back. Expected behavior: The client should now see a new bubble, being updated with the current step, so it looks like something is happening. When the pipeline finishes and the final response is streamed, it should be streamed on the client side as the chat response, in a single bubble. Only the answer