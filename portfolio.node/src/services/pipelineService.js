const { queryLLMByName } = require("../services/llmService");
const { fetchRelevantData } = require("../services/dataFetcherService");
const { generateEmbeddings } = require("../services/embeddingService");

/**
 * Executes an AI query pipeline.
 */
async function executePipeline(promptName, parameters, isStreaming = false, streamCallback = null) {
    try {
        // if (isStreaming && streamCallback) streamCallback({ response: "ğŸ”„ Starting AI pipeline...", step: "starting" });

        if (isStreaming && streamCallback) streamCallback({ response: "ğŸ” Analyzing prompt...", step: "starting" });

        // Step 1ï¸âƒ£: Detect Intent
        if (isStreaming && streamCallback) streamCallback({ response: "ğŸ” Detecting user intent...", step: "starting" });
        const intentResponse = await queryLLMByName("intent-detection", { userQuery: parameters.userQuery }, false);
        const intent = intentResponse?.intent || "general_knowledge"; 

        if (isStreaming && streamCallback) streamCallback({ response: `ğŸ¯ Identified intent: ${intent}`, step: "starting" });

        // Step 2ï¸âƒ£: Convert User Query to Vector for Qdrant
        if (isStreaming && streamCallback) streamCallback({ response: "ğŸ§  Generating query embeddings...", step: "starting" });
        parameters.queryVector = await generateEmbeddings(parameters.userQuery);

        // Step 3ï¸âƒ£: Fetch Relevant Data
        if (isStreaming && streamCallback) streamCallback({ response: `ğŸ“¡ Retrieving relevant ${intent} data...`, step: "starting" });
        parameters.context = await fetchRelevantData(parameters, intent);

        if (isStreaming && streamCallback) streamCallback({ response: "ğŸ¤– Generating AI response...", step: true });

        const grResponse = await queryLLMByName("guardrail", parameters, false);
        const block = grResponse?.block || "no"; 
        console.log(`guardrail response`, grResponse);
        if ( block == "yes"){
            if (isStreaming && streamCallback) streamCallback({ response: `Sorry, I am not allowed to answer this. ${grResponse.reason || "" }`, step: false, done: true });
            return { response: `Sorry, I am not allowed to answer this. ${grResponse.reason}`, step: false, done: true };
        }

        // Step 4ï¸âƒ£: Execute AI Query

        const responseStream = await queryLLMByName(promptName, parameters, true);

        const reader = responseStream.getReader();
        let responseBuffer = "";

        while (true) {
            const { done, value } = await reader.read();
            if (done) {
                streamCallback({ response: responseBuffer, done: true, step: false });
                break;
            }

            responseBuffer += value;

            // âœ… Stream words immediately but separate paragraphs
            if (value.includes(" ")) {
                streamCallback({ response: value, step: false });
            }

            if (value.includes("\n\n")) {
                streamCallback({ response: "ğŸ”½ New Paragraph ğŸ”½", step: false }); // Placeholder to mark paragraph shift
            }
        }

        return { response: "Streaming response handled separately." };
    } catch (error) {
        if (isStreaming && streamCallback) streamCallback({ response: `âŒ Error: ${error.message}` });
        return { response: "An error occurred while processing your request." };
    }
}

module.exports = { executePipeline };
